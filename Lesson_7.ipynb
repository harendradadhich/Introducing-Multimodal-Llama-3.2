{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cc6594-88b5-4728-96c7-d5563b07be28",
   "metadata": {},
   "source": [
    "# Lab 7: Llama Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20efa5ef-5484-4b9d-b31d-2166cc35f876",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303897a5-2dbc-4d0c-af34-a35769f8b9b1",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd945c34-f7d4-41ef-8c57-2150f5c8b7cb",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "_ = load_dotenv() #loads 'TOGETHER_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d6f57-cf6a-4b1a-9187-83ea5617f09a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "#!pip install llama-stack==0.1.0 llama-stack-client==0.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e318db-d0c3-4d6c-b3d6-f11832574e93",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> üíª &nbsp; <b>Access <code>requirements.txt</code> and <code>utils.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456331d9-0be8-45b8-bdf6-c55e9772075e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "!llama stack build --list-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632e7d6-b4ab-4d97-8ac5-3e962207a9fb",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "!llama stack list-apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8d6d3-de76-436e-96d3-b54929a32218",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "!llama-stack-client configure --endpoint https://llama-stack.together.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788dadb-aeb9-4a8f-bb3b-9f420889f7bb",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "!llama-stack-client models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f455a-5bef-4a4d-9689-6be2c1772c40",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(base_url=f\"https://llama-stack.together.ai\")\n",
    "\n",
    "models = client.models.list()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2c493-8cb6-4011-ab2a-07bc64691058",
   "metadata": {},
   "source": [
    "#  Llama Stack Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed253e6-68ca-4f99-ae2c-fd4387c15401",
   "metadata": {
    "height": 419
   },
   "outputs": [],
   "source": [
    "LLAMA_STACK_API_TOGETHER_URL=\"https://llama-stack.together.ai\"\n",
    "LLAMA31_8B_INSTRUCT = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "import json\n",
    "\n",
    "def run_main():\n",
    "    client = LlamaStackClient(\n",
    "        base_url=LLAMA_STACK_API_TOGETHER_URL,\n",
    "    )\n",
    "\n",
    "    response = client.inference.chat_completion(\n",
    "        model_id=LLAMA31_8B_INSTRUCT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Who wrote the book Innovator's Dilemma? How about Charlotte's Web?\"},\n",
    "            {\"role\": \"user\", \"content\": \"which book was published first?\"}            \n",
    "        ],\n",
    "        x_llama_stack_provider_data=json.dumps({\"together_api_key\": os.getenv('TOGETHER_API_KEY')})\n",
    "    )\n",
    "\n",
    "    print(response.completion_message.content)\n",
    "    \n",
    "run_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9dca1-3536-4637-a46a-a80d2776adad",
   "metadata": {
    "id": "JSOezWytd0pA"
   },
   "source": [
    "# Llama Stack Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1457c6d-dd5c-46ad-a565-569067a4f2cd",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.types.agent_create_params import AgentConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0dd18f-316a-43eb-87f4-5dec8568f65a",
   "metadata": {
    "height": 608
   },
   "outputs": [],
   "source": [
    "async def run_main():\n",
    "    client = LlamaStackClient(\n",
    "        base_url=LLAMA_STACK_API_TOGETHER_URL,\n",
    "    )    \n",
    "    \n",
    "    agent_config = AgentConfig(\n",
    "        model=LLAMA31_8B_INSTRUCT,\n",
    "        instructions=\"You are a helpful assistant\",\n",
    "        enable_session_persistence=False,\n",
    "    )\n",
    "\n",
    "    agent = Agent(client, agent_config)\n",
    "    session_id = agent.create_session(\"test-session\")\n",
    "\n",
    "    prompts = [\n",
    "        \"Who wrote the book Charlotte's Web?\",\n",
    "        \"Three best quotes?\",\n",
    "    ]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"User> {prompt}\")\n",
    "        response = agent.create_turn(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            session_id=session_id,\n",
    "        )\n",
    "\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "        \n",
    "await run_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1871348-8265-4a8a-8add-d682fb9c7605",
   "metadata": {
    "id": "IPVNKOxYwvKH"
   },
   "source": [
    "# Llama Stack with Llama 3.2 vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879128d7-3b7e-411a-a099-0345a023d14d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "height": 183,
    "id": "wxNRgPhg8o9X",
    "outputId": "7854bed8-ed54-4ecf-abf2-d1b822c3330b"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image(path):\n",
    "  img = Image.open(path)\n",
    "  plt.imshow(img)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "display_image(\"./content/Llama_Repo.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b748c08-8b25-4fcf-a263-adf3878695cc",
   "metadata": {
    "height": 846
   },
   "outputs": [],
   "source": [
    "LLAMA32_11B_INSTRUCT = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        base64_url = f\"data:image/png;base64,{base64_string}\"\n",
    "        return base64_url\n",
    "\n",
    "async def run_main(image_path, prompt):\n",
    "    base64_image = encode_image(image_path)\n",
    "    \n",
    "    client = LlamaStackClient(\n",
    "        base_url=LLAMA_STACK_API_TOGETHER_URL,\n",
    "    )    \n",
    "    \n",
    "    agent_config = AgentConfig(\n",
    "        model=LLAMA32_11B_INSTRUCT,\n",
    "        instructions=\"You are a helpful assistant\",\n",
    "        enable_session_persistence=False,\n",
    "    )\n",
    "\n",
    "    agent = Agent(client, agent_config)\n",
    "    session_id = agent.create_session(\"test-session\")\n",
    "\n",
    "    response = agent.create_turn(\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": {\n",
    "                         \"url\": {\n",
    "                              \"uri\": encode_image(image_path)\n",
    "                         }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt,\n",
    "                }\n",
    "            ]\n",
    "        }],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "\n",
    "    for log in EventLogger().log(response):\n",
    "        log.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d3b17-9823-4b5e-9b2f-2a53f7b4be24",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "await run_main(\"./content/Llama_Repo.jpeg\",\n",
    "         \"How many different colors are those llamas? What are those colors?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309c425-f869-4885-8933-b728b2c76a7c",
   "metadata": {
    "height": 608
   },
   "outputs": [],
   "source": [
    "import mimetypes\n",
    "from termcolor import cprint\n",
    "from llama_stack_client.lib.inference.event_logger import EventLogger\n",
    "\n",
    "async def run_main(image_path: str, prompt):\n",
    "    client = LlamaStackClient(\n",
    "        base_url=LLAMA_STACK_API_TOGETHER_URL,\n",
    "    )    \n",
    "\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": {\n",
    "                     \"url\": {\n",
    "                          \"uri\": encode_image(image_path)\n",
    "                     }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt,\n",
    "            }\n",
    "        ]       \n",
    "    }\n",
    "\n",
    "    cprint(\"User> Sending image for analysis...\", \"green\")\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=[message],\n",
    "        model_id=LLAMA32_11B_INSTRUCT,\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    print(response.completion_message.content.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4b553-a117-4f92-9225-ac2b2483ac0b",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "await run_main(\"./content/Llama_Repo.jpeg\",\n",
    "     \"How many different colors are those llamas?\\\n",
    "     What are those colors?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de0299-a25b-4f78-bf28-7963446c68dd",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
